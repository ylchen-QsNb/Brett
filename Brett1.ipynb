{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8auxly2MVx_v"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP1aJMJx2b41UQscV9QDbJ9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ylchen-QsNb/Brett/blob/main/Brett1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ppg8ELpEWa0A"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "emb_size = 768\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('which device we are using: ' + device)\n",
        "\n",
        "class SSM(nn.Module):\n",
        "  def __init__(self, d_state, d_out, d_a, hidden):\n",
        "    super().__init__()\n",
        "\n",
        "    self.state_ini = nn.Linear(d_out, d_state, bias=False)\n",
        "    self.get_ini = torch.eye(d_out, d_out, requires_grad=False).to(device)\n",
        "\n",
        "    self.alpha = nn.Sequential(\n",
        "        nn.Linear(d_state, hidden),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(hidden, hidden),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(hidden, d_a),\n",
        "        nn.Softmax(dim=-1)\n",
        "    )\n",
        "    self.beta = nn.Sequential(\n",
        "        nn.Linear(d_state, hidden),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(hidden, hidden),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(hidden, d_a),\n",
        "        nn.Softmax(dim=-2)\n",
        "    )\n",
        "    self.gamma = nn.Sequential(\n",
        "        nn.Linear(d_state, hidden),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(hidden, hidden),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(hidden, d_state),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "    self.delta = nn.Linear(d_state, d_state, bias=False)\n",
        "    self.A = nn.Parameter(torch.randn(d_out, d_state))\n",
        "    self.B = nn.Parameter(torch.randn(d_out, d_state))\n",
        "    self.tau = nn.Parameter(torch.rand(d_out, d_state))\n",
        "\n",
        "\n",
        "  def initialize(self):\n",
        "    return self.state_ini(self.get_ini)\n",
        "\n",
        "  def forward(self, d_t, state):\n",
        "    beta = self.beta(state)\n",
        "    gamma = self.gamma(state)\n",
        "    bg = torch.matmul(torch.transpose(beta, -1, -2), gamma)\n",
        "    bgd = self.delta(bg)\n",
        "    alpha = self.alpha(state)\n",
        "    abgd = torch.matmul(alpha, bgd)\n",
        "\n",
        "    out = (state + d_t * (abgd*self.A + self.B)) / (1 + d_t * (self.tau + abgd))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# Brett: Bidirectional Recurssive Encoder from Transformer Time-dependent\n",
        "class BrettCore(nn.Module):\n",
        "  def __init__(self, d_model, head, d_k, d_v, d_state, d_a, hidden, hidden_feed):\n",
        "    super().__init__()\n",
        "    self.head = head\n",
        "    self.d_model = d_model\n",
        "    self.d_k = d_k\n",
        "    self.d_v = d_v\n",
        "    self.d_state = d_state\n",
        "    \n",
        "    self.ssm_q = nn.ModuleList(\n",
        "        [SSM(d_state, d_k, d_a, hidden) for _ in range(head)]\n",
        "    )\n",
        "    self.ssm_k = nn.ModuleList(\n",
        "        [SSM(d_state, d_k, d_a, hidden) for _ in range(head)]\n",
        "    )\n",
        "    self.ssm_v = nn.ModuleList(\n",
        "        [SSM(d_state, d_v, d_a, hidden) for _ in range(head)]\n",
        "    )\n",
        "\n",
        "    self.sq = nn.ModuleList(\n",
        "        [nn.Linear(d_state, d_model) for _ in range(head)]\n",
        "    )\n",
        "    self.sk = nn.ModuleList(\n",
        "        [nn.Linear(d_state, d_model) for _ in range(head)]\n",
        "    )\n",
        "    self.sv = nn.ModuleList(\n",
        "        [nn.Linear(d_state, d_model) for _ in range(head)]\n",
        "    )\n",
        "\n",
        "    self.feed_a_state = SSM(2*hidden_feed, hidden_feed, hidden_feed//2, hidden_feed)\n",
        "    self.feed_a_trans = nn.Linear(2*hidden_feed, head*d_v)\n",
        "    self.feed_a_bias = nn.Linear(2*hidden_feed, 1)\n",
        "\n",
        "    self.gelu = nn.GELU()\n",
        "\n",
        "    self.feed_b_state = SSM(2*d_model, d_model, d_model//2, d_model)\n",
        "    self.feed_b_trans = nn.Linear(2*d_model, hidden_feed)\n",
        "    self.feed_b_bias = nn.Linear(2*d_model, 1)\n",
        "\n",
        "    self.tau = nn.Parameter(torch.rand(1, 1, d_model))\n",
        "\n",
        "  def initialize(self):\n",
        "    state_q = torch.zeros(0, self.d_k, self.d_state).to(device)\n",
        "    state_k = torch.zeros(0, self.d_k, self.d_state).to(device)\n",
        "    state_v = torch.zeros(0, self.d_v, self.d_state).to(device)\n",
        "    for idx in range(self.head):\n",
        "      state_q = torch.cat((state_q, self.ssm_q[idx].initialize().unsqueeze(0)), 0)\n",
        "      state_k = torch.cat((state_k, self.ssm_k[idx].initialize().unsqueeze(0)), 0)\n",
        "      state_v = torch.cat((state_v, self.ssm_v[idx].initialize().unsqueeze(0)), 0)\n",
        "    state_a = self.feed_a_state.initialize()\n",
        "    state_b = self.feed_b_state.initialize()\n",
        "    out_tuple = (state_q, state_k, state_v, state_a, state_b)\n",
        "    return out_tuple\n",
        "\n",
        "  def forward(self, dt, x, in_tuple, mask=None):\n",
        "    batch = x.size(0)\n",
        "    length = x.size(1)\n",
        "\n",
        "    state_q, state_k, state_v, state_a, state_b = in_tuple\n",
        "\n",
        "    if mask is not None:\n",
        "      pad = mask.unsqueeze(-1)\n",
        "      pad = pad.repeat(1, 1, self.d_k)\n",
        "\n",
        "    attn = torch.zeros(batch, length, 0).to(device)\n",
        "\n",
        "    out_q = torch.zeros(0, self.d_k, self.d_state).to(device)\n",
        "    out_k = torch.zeros(0, self.d_k, self.d_state).to(device)\n",
        "    out_v = torch.zeros(0, self.d_v, self.d_state).to(device)\n",
        "\n",
        "    for idx in range(self.head):\n",
        "      wq_trans = self.sq[idx](state_q[idx, :, :])\n",
        "      wk_trans = self.sq[idx](state_k[idx, :, :])\n",
        "      wv_trans = self.sq[idx](state_v[idx, :, :])\n",
        "\n",
        "      k_trans = torch.matmul(wk_trans, torch.transpose(x, -1, -2))\n",
        "\n",
        "      if mask is not None:\n",
        "        padd = torch.transpose(pad, -1, -2)\n",
        "        k_trans = k_trans.masked_fill(padd==0, -1e9)\n",
        "\n",
        "      k_trans = nn.functional.softmax(k_trans, dim=-1)\n",
        "      v = torch.matmul(x, torch.transpose(wv_trans, -1, -2))\n",
        "      ktv = torch.bmm(k_trans, v)\n",
        "      q = torch.matmul(x, torch.transpose(wq_trans, -1, -2))\n",
        "\n",
        "      if mask is not None:\n",
        "        q = q.masked_fill(pad==0, -1e9)\n",
        "\n",
        "      q = nn.functional.softmax(q, dim=-1)\n",
        "      qktv = torch.matmul(q, ktv)\n",
        "\n",
        "      attn = torch.cat((attn, qktv), -1)\n",
        "\n",
        "      out_q = torch.cat((out_q, self.ssm_q[idx].forward(dt, state_q[idx, :, :]).unsqueeze(0)), 0)\n",
        "      out_k = torch.cat((out_k, self.ssm_k[idx].forward(dt, state_k[idx, :, :]).unsqueeze(0)), 0)\n",
        "      out_v = torch.cat((out_v, self.ssm_v[idx].forward(dt, state_v[idx, :, :]).unsqueeze(0)), 0)\n",
        "    \n",
        "    out_a = self.feed_a_state.forward(dt, state_a)\n",
        "    out_b = self.feed_b_state.forward(dt, state_b)\n",
        "    \n",
        "    # f = self.feed(attn)\n",
        "\n",
        "    f = torch.matmul(attn, torch.transpose(self.feed_a_trans(state_a), -1, -2))\n",
        "    f = f + torch.transpose(self.feed_a_bias(state_a), -1, -2)\n",
        "    f = self.gelu(f)\n",
        "    f = torch.matmul(f, torch.transpose(self.feed_b_trans(state_b), -1, -2))\n",
        "    f = f + torch.transpose(self.feed_b_bias(state_b), -1, -2)\n",
        "    denom = torch.sum(f*x, -1).unsqueeze(-1) / self.d_model + self.tau\n",
        "    y = (x + dt*f) / (1 + dt*denom)\n",
        "    out_tuple = (out_q, out_k, out_v, out_a, out_b)\n",
        "    return y, out_tuple\n",
        "\n",
        "class Embed(nn.Module):\n",
        "  def __init__(self, vec_size=emb_size, dict_size=30522, max_len=512, max_sentence=32):\n",
        "    super().__init__()\n",
        "    self.word_embed = nn.Embedding(dict_size, vec_size, padding_idx=0)\n",
        "    self.position_embed = nn.Embedding(max_len, vec_size, padding_idx=0)\n",
        "    self.sentence_embed = nn.Embedding(max_sentence, vec_size, padding_idx=0)\n",
        "    \n",
        "  def forward(self, token):\n",
        "    # token = self.tokenizer(text1, text2, padding=True, truncation=True, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    attn_mask = token['attention_mask'].to(device)\n",
        "    word_ids = token['input_ids'].to(device)\n",
        "    type_ids = token['token_type_ids'].to(device)\n",
        "    length = word_ids.size(-1)\n",
        "    batch = word_ids.size(0)\n",
        "    position_ids = torch.arange(length).repeat(batch, 1).to(device)\n",
        "    vector = self.word_embed(word_ids) + self.position_embed(position_ids) + self.sentence_embed(type_ids)\n",
        "\n",
        "    return vector, attn_mask\n",
        "\n",
        "class Brett(nn.Module):\n",
        "  def __init__(self, d_model, head, d_k, d_v, d_state, d_a, hidden, hidden_feed):\n",
        "    super().__init__()\n",
        "    self.embed = Embed()\n",
        "    self.brett = BrettCore(d_model, head, d_k, d_v, d_state, d_a, hidden, hidden_feed)\n",
        "\n",
        "  def forward(self, token, num=11):\n",
        "    x, pad = self.embed(token)\n",
        "    print(x.shape)\n",
        "    samples = torch.rand(num)\n",
        "    samples, _ = torch.sort(samples)\n",
        "    samples = torch.cat((samples, torch.tensor([1.0])), -1)\n",
        "    t0 = torch.tensor(0.0)\n",
        "    state = self.brett.initialize()\n",
        "    for t1 in samples:\n",
        "      dt = t1 - t0\n",
        "      print('t1:', t1, 'dt:', dt)\n",
        "      x, state = self.brett.forward(dt, x, state, mask=pad)\n",
        "      t0 = t1\n",
        "    return x\n",
        "\n",
        "class Pretrain_Brett(nn.Module):\n",
        "  def __init__(self, d_model, head, d_k, d_v, d_state, d_a, hidden, hidden_feed):\n",
        "    super().__init__()\n",
        "    self.brett = Brett(d_model, head, d_k, d_v, d_state, d_a, hidden, hidden_feed)\n",
        "    self.mlm_out = nn.Sequential(\n",
        "        nn.Linear(d_model, 30522),\n",
        "    )\n",
        "    self.nsp_out = nn.Sequential(\n",
        "        nn.Linear(d_model, d_model),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(d_model, 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    # self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "    self.loss = nn.BCELoss()\n",
        "    self.loss_mlm = nn.CrossEntropyLoss(ignore_index=0)\n",
        "    \n",
        "  def nsp(self, token, target, num=7):\n",
        "    out = self.brett(token, num=num)\n",
        "    classifier_units = out[:, 0, :]\n",
        "    nsp_res = self.nsp_out(classifier_units).squeeze()\n",
        "    # target = torch.tensor(target)\n",
        "    # target = target.to(device)\n",
        "    # print(device, target.device)\n",
        "    loss = self.loss(nsp_res, target)\n",
        "    return loss\n",
        "\n",
        "  def mlm(self, masked_token, cheat_sheet, num=7):\n",
        "    out = self.brett(masked_token, num=num)\n",
        "    logits = self.mlm_out(out)\n",
        "    # loss = 0\n",
        "    # for idx in range(cheat_sheet.size(0)):\n",
        "    #   testant = out[idx, :, :]\n",
        "    #   mask = cheat_sheet[idx, :]==1\n",
        "    #   testant_ = torch.masked_select(testant, mask[:, None]).reshape(-1, testant.shape[1])\n",
        "    #   mlm_res = self.mlm_out(testant_)\n",
        "    #   test_token = origin_token[idx, :][mask]\n",
        "    #   res = torch.diag(torch.index_select(mlm_res, dim=1, index=test_token))\n",
        "    #   loss = loss + torch.mean(torch.log(res))\n",
        "    logits = logits.view(-1, 30522)\n",
        "    labels = cheat_sheet.view(-1).to(device)\n",
        "\n",
        "    return self.loss_mlm(logits, labels)\n",
        "\n",
        "def pre_nsp(training_set):\n",
        "  tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "  length = len(training_set)\n",
        "  set1 = []\n",
        "  set2 = []\n",
        "  target = []\n",
        "  for idx in range(length-1):\n",
        "    set1.append(training_set[idx])\n",
        "    if idx % 2 == 0:\n",
        "      set2.append(training_set[idx+1])\n",
        "      target.append(1.0)\n",
        "    else:\n",
        "      id = random.randint(0, length-1)\n",
        "      if id == idx+1:\n",
        "        while id == idx+1:\n",
        "          id = random.randint(0, length-1)\n",
        "      set2.append(training_set[id])\n",
        "      target.append(0.0)\n",
        "  \n",
        "  token = tokenizer(set1, set2, padding=True, truncation=True, add_special_tokens=True, return_tensors=\"pt\")\n",
        "  return token, torch.tensor(target)\n",
        "\n",
        "def pre_mlm(training_set):\n",
        "  tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "  length = len(training_set)\n",
        "\n",
        "  token = tokenizer(training_set, padding=True, truncation=True, add_special_tokens=True, return_tensors=\"pt\")\n",
        "  origin_token = token['input_ids']\n",
        "  cheat_sheet = torch.zeros(origin_token.size(0), origin_token.size(1), dtype=torch.long)\n",
        "\n",
        "  for j in range(length):\n",
        "    con_len = torch.count_nonzero(origin_token[j, :])\n",
        "    rand = random.sample(range(con_len), con_len*3//20)\n",
        "    for idx in range(con_len*3//20):\n",
        "      enter = rand[idx]\n",
        "      u = random.random()\n",
        "      if u < 0.8:\n",
        "        token['input_ids'][j, enter] = tokenizer.mask_token_id\n",
        "        cheat_sheet[j, enter] = tokenizer.mask_token_id\n",
        "      elif (u > 0.8) and (u < 0.9):\n",
        "        replace = random.randint(tokenizer.mask_token_id+1, 30522)\n",
        "        token['input_ids'][j, enter] = replace\n",
        "        cheat_sheet[j, enter] = replace\n",
        "\n",
        "  return token, cheat_sheet"
      ],
      "metadata": {
        "id": "EoKGwE-dYDL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main()"
      ],
      "metadata": {
        "id": "RBPoVS0jf8Hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_nsp(pretrain, training_set, epochs, lr):\n",
        "  token, target = pre_nsp(training_set)\n",
        "  token = token.to(device)\n",
        "  target = target.to(device)\n",
        "  optimizer = torch.optim.Adam(pretrain.parameters(), lr=lr)\n",
        "  for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    loss = pretrain.nsp(token, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print('epoch: {}/{}, loss: {}'.format(epoch+1, epochs, loss.item()))\n",
        "\n",
        "def train_mlm(pretrain, training_set, epochs, lr):\n",
        "  token, cheat_sheet = pre_mlm(training_set)\n",
        "  token = token.to(device)\n",
        "  print(cheat_sheet)\n",
        "  cheat_sheet = cheat_sheet.to(device)\n",
        "  optimizer = torch.optim.Adam(pretrain.parameters(), lr=lr)\n",
        "  for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    loss = pretrain.mlm(token, cheat_sheet)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print('epoch: {}/{}, loss: {}'.format(epoch+1, epochs, loss.item()))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  d_model, head, d_k, d_v, d_state, d_a, hidden, hidden_feed = 768, 64, 64, 128, 128, 32, 128, 128\n",
        "  pretrain = Pretrain_Brett(d_model, head, d_k, d_v, d_state, d_a, hidden, hidden_feed).to(device)\n",
        "  batch_size = 64\n",
        "\n",
        "  training_set = [\n",
        "      \"I'm good to go for the party.\",\n",
        "      \"I have no idea where is the location?\",\n",
        "      \"I can guide you guys to the rigth place.\",\n",
        "      \"Thank you so much!\",\n",
        "      \"No worries, we are friends.\",\n",
        "      \"Have a fun time!\"\n",
        "  ]\n",
        "\n",
        "  epochs = 8\n",
        "  lr = 5e-5\n",
        "  train_nsp(pretrain, training_set, epochs, lr)"
      ],
      "metadata": {
        "id": "nQMrVXCqYXkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trash Bin"
      ],
      "metadata": {
        "id": "8auxly2MVx_v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jc_NnDHcVzmq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}